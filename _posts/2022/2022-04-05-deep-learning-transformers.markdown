---
layout: post
title: Deep Learning - Transformers
date: '2022-04-05 13:19'
subtitle: TODO
comments: true
header-img: "img/home-bg-art.jpg"
tags:
    - Deep Learning
---



![Screenshot from 2024-11-17 15-52-10](https://github.com/user-attachments/assets/c293b115-a8f8-42a0-9589-fe6a1b200beb)

- A Transformer Network processes sentences from left to right, one word at a time.

- What does the output of the encoder block contain?
- Why is positional encoding important in the translation process? (Check all that apply)
- Which of these is not a good criterion for a good positional encoding algorithm?
  - must be deterministic
  - can generalize to longer sentences
  - A common encoding for each timestep (words position in a sentence)
