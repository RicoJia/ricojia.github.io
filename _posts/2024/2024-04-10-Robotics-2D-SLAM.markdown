---
layout: post
title: Robotics - 2D SLAM
date: '2024-04-10 13:19'
subtitle: Scan Matching, ICP, 
comments: true
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Robotics
    - SLAM
---

## Introduction

The assumption of a robot moving on a 2D plane is a strong one. However, many indoor robots do have support an assumption. Maps in this case can simply be interpreted as an image. Low-end 2D Lidars use a belt to turn its head. Hotel robots and cleaning robots usually use solid-state Lidars.

When the robots do need to perceive heights outside of its 2D Lidar's range, we usually either add an RGBD camera, or manually label on the map for restrcited areas.

- Labelling is more difficult in the 3D world. One example is labelling **heights of traffic lights**. In most cases, labelling is done on 2D.

In circa.2007, 2D SLAM was a hit. Sebastian Thrun and Gmapping authors: Wolfram Burgard, Cyrill Stachniss, and G Grisetti are some big names there. EKF SLAM family and the Particle SLAM Family (Fast-SLAM, RBPF SLAM) were popular. Their problem is that once the map is crooked, we can't easily fix them. 

Early days: no front end or back end; only one map is saved; Simple Loop closure detection;
Modern 2D SLAM: Pose Graph is used as backend; Key frames or sub-maps are the basic process units; Guarantees complete map after loop closure

| Early Days SLAM                           | Modern 2D SLAM                                      |
|-------------------------------------------|-----------------------------------------------------|
| No front end or back end                  | Pose Graph is used as backend                      |
| Only one map is saved                      | Key frames or sub-maps are the basic process units |
| Simple loop closure detection              | Guarantees complete map after loop closure         |


Concepts:
- Scan is a complete 360 deg LiDAR scan
- Scan matching is to find the relative pose between a scan vs another scan / map. **Scan matching is the core technique in a 2D SLAM**
- Submap: the map generated by several scans
- Occupancy gid: probability representation of a 2D map

<div style="text-align: center;">
    <p align="center">
       <figure>
            <img src="https://github.com/user-attachments/assets/5d26cec9-fef6-4cf0-8660-26abc3f9b3f5" height="300" alt=""/>
       </figure>
    </p>
</div>

## Scan Matching

The goal of scan matching is: given the observation model $z = h(x, u) + w$, find the most likely estimate 

$$
\begin{gather*}
\begin{aligned}
& X_{MLE} = argmax_{x}(x | z, m) = argmax_{x}(z | x, m)
\end{aligned}
\end{gather*}
$$

- Where `m` is thelast scan

To calculate which grid cells a ray goes through, we need ray casting (光线投射算法) and rasterization (栅格算法)

There are two main ways to do it:
- ICP (ICP, Point-to-Line ICP (PL_ICP), GICP, ICL): 2D and 3D are similar
- Likelihood fields (Gaussian Likelihood field, CSM)

The main problems in scan matching are:
- Which points should we perform scan matching on? 
    - All points for 2D
    - Sampling for 3D (according normal vectors, features, etc.)
- How to find point association? (KNN)
- How to find residuals. For scan point `[x,y]_i` and its matching point on another scan `[x, y]'_j`, we want to calculate its error (difference). The complete beam model is complicated and is not smooth for state estimation. We usually simplify the difference into a 2D Gaussian Distribution.

### 2D Iterative Closest Point (ICP)

In 2D, robot pose is `[x, y, theta]`. The coordinate system we use are $T_{WB}$, and Later, sub map frame. A typical ICP-like algorithm iterative conducts 2 steps:

1. Data association
2. Pose estimation

A single 2D LiDAR Point comes in as $[\phi, r]$:

$$
\begin{gather*}
\begin{aligned}
& p_i^w = [x + r_i cos(\phi_i + \theta), y + r_i sin(\phi_i + \theta)]
\end{aligned}
\end{gather*}
$$

We can define **error as the cartesian difference between a specific point and its nearest neighbor in the other point cloud**:

$$
\begin{gather*}
\begin{aligned}
& e_i = p_i^w - q_i^w
\end{aligned}
\end{gather*}
$$

Then, the scan matching problem becomes a non-linear least-squares optimization problem:

$$
\begin{gather*}
\begin{aligned}
& (x, y, \theta)* = argmin(\sum_I |e_i|^2)
\end{aligned}
\end{gather*}
$$

This least squares problem is not linear (because of cos and sin in `p_i`), but it can be resolved by a Gauss-Newton minimizer like G2O. Therefore, we need the partial direvatives of each individual cost w.r.t `[x, y, theta]`:

$$
\begin{gather*}
\begin{aligned}
& \frac{\partial e_i}{\partial x} = [1, 0]
\\ &
\frac{\partial e_i}{\partial y} = [0, 1]
\\ &
\frac{\partial e_i}{\partial \theta} = [-r_i sin(\phi_i + \theta), r_i cos(\phi_i + \theta)]

\\ &
\Rightarrow
 \frac{\partial e_i}{\partial x} = \begin{bmatrix}
 1 & 0 \\
 0 & 1 \\
 -r_i sin(\phi_i + \theta) & r_i cos(\phi_i + \theta)
 \end{bmatrix}

\end{aligned}
\end{gather*}
$$

One advantage of 2D pose is its angles are additive, so it's not like 3D where we need to use `SO(3)` manifolds.

**Note that during optimization, after updating [x, y, theta], point correspondence would likely change**. So ICP is reliant on the initial relative pose estimate. In the below image, when the initial pose esimate is far off, point correspondence could be wrong:

<div style="text-align: center;">
    <p align="center">
       <figure>
            <img src="https://github.com/user-attachments/assets/e2ac93ed-086f-43eb-864f-7415be0ed571" height="300" alt=""/>
       </figure>
    </p>
</div>