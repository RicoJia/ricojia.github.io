---
layout: post
title: Robotics - 2D SLAM
date: '2024-04-10 13:19'
subtitle: 
comments: true
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Robotics
    - SLAM
---

## Introduction

The assumption of a robot moving on a 2D plane is a strong one. However, many indoor robots do have support an assumption. Maps in this case can simply be interpreted as an image. Low-end 2D Lidars use a belt to turn its head. Hotel robots and cleaning robots usually use solid-state Lidars.

When the robots do need to perceive heights outside of its 2D Lidar's range, we usually either add an RGBD camera, or manually label on the map for restrcited areas.

- Labelling is more difficult in the 3D world. One example is labelling **heights of traffic lights**. In most cases, labelling is done on 2D.

In circa.2007, 2D SLAM was a hit. Sebastian Thrun and Gmapping authors: Wolfram Burgard, Cyrill Stachniss, and G Grisetti are some big names there. EKF SLAM family and the Particle SLAM Family (Fast-SLAM, RBPF SLAM) were popular. Their problem is that once the map is crooked, we can't easily fix them. 

Early days: no front end or back end; only one map is saved; Simple Loop closure detection;
Modern 2D SLAM: Pose Graph is used as backend; Key frames or sub-maps are the basic process units; Guarantees complete map after loop closure

| Early Days SLAM                           | Modern 2D SLAM                                      |
|-------------------------------------------|-----------------------------------------------------|
| No front end or back end                  | Pose Graph is used as backend                      |
| Only one map is saved                      | Key frames or sub-maps are the basic process units |
| Simple loop closure detection              | Guarantees complete map after loop closure         |


Concepts:
- Scan is a complete 360 deg LiDAR scan
- Scan matching is to find the relative pose between a scan vs another scan / map. **Scan matching is the core technique in a 2D SLAM**
- Submap: the map generated by several scans
- Occupancy gid: probability representation of a 2D map

<div style="text-align: center;">
    <p align="center">
       <figure>
            <img src="https://github.com/user-attachments/assets/5d26cec9-fef6-4cf0-8660-26abc3f9b3f5" height="300" alt=""/>
       </figure>
    </p>
</div>

## Scan Matching

The goal of scan matching is: given the observation model $z = h(x, u) + w$, find the most likely estimate 

$$
\begin{gather*}
\begin{aligned}
& X_{MLE} = argmax_{x}(x | z, u) = argmax_{x}(z | x, u)
\end{aligned}
\end{gather*}
$$

There are two main ways to do it:
- ICP (ICP, PL_ICP, GICP, ICL): 2D and 3D are similar
- Likelihood fields (Gaussian Likelihood field, CSM)

The main problems in scan matching are:
- Which points should we perform scan matching on? (all points for 2D)
- How to find point association? (KNN)
- How to find residuals 

### 2D ICP

In 2D, robot pose is `[x, y, theta]`. The coordinate system we use are $T_{WB}$, and Later, sub map frame. For a single 2D LiDAR Point, it comes in as $[\phi, r]$:

$$
\begin{gather*}
\begin{aligned}
& p_i^w = [x + r_i cos(\phi_i + \theta), y + r_i sin(\phi_i + \theta)]
\end{aligned}
\end{gather*}
$$

We can define error as the difference between a specific point and its nearest neighbor in the other point cloud:

$$
\begin{gather*}
\begin{aligned}
& e_i = p_i^w - q_i^w
\end{aligned}
\end{gather*}
$$

The partial direvatives are:

$$
\begin{gather*}
\begin{aligned}
& \frac{\partial e_i}{\partial x} = [1, 0]
\\ &
\frac{\partial e_i}{\partial y} = [0, 1]
\\ &
\frac{\partial e_i}{\partial \theta} = [-r_i sin(\phi_i + \theta), r_i cos(\phi_i + \theta)]

\\ &
\Rightarrow
 \frac{\partial e_i}{\partial x} = \begin{bmatrix}
 1 & 0 \\
 0 & 1 \\
 -r_i sin(\phi_i + \theta) & r_i cos(\phi_i + \theta)
 \end{bmatrix}

\end{aligned}
\end{gather*}
$$